{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7ec47984",
   "metadata": {},
   "source": [
    "---\n",
    "self-contained: true\n",
    "title: \"Advanced Income Forecasting Using Machine Learning Models\"\n",
    "author: \"Riley Svensson\"\n",
    "format:\n",
    "    html:\n",
    "        theme: journal\n",
    "        toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c445883",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This analysis focuses on predicting an individual's income level using machine learning modeling techniques. By employing methods like Naive Bayes, Neural Networks, and Decision Trees, we aim to understand how various features like marital status or occupation influence whether a person falls into a higher or lower income bracket.  Specifically, our goal is to develop a model capable of predicting whether an individual earns over or under $50,000 annually, which could have numerous applications, from targeted marketing strategies to socio-economic research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fa69a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages and potential data science tools like pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from imblearn.pipeline import Pipeline as ImblearnPipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPRegressor,MLPClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB, ComplementNB\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf33b4",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before we can apply machine learning models, it's crucial to prepare our data. This involves tasks like handling missing values, encoding categorical variables, and normalizing features. These steps ensure that our models receive clean and structured data, which leads to more accurate predictions.  In this case, I identified several differing data types, including objects and integers, that needed to be addressed before modeling. I also resolved any instances of special characters (e.g., ?) in the data to prevent noise and avoid potential errors. The most important step was transforming the Income column into a binary (dummified) column, where a value of 1 indicates an income greater than 50,000, and 0 indicates an income of 50,000 or less. This transformation simplified the preprocessing required later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60538bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in the data, naming our df purposefully\n",
    "income = pd.read_csv(\"/Users/rileysvensson/Desktop/GSB 545 - Advanced Machine Learning/data /income_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f15395e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education-num     0\n",
       "marital-status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital-gain      0\n",
       "capital-loss      0\n",
       "hours-per-week    0\n",
       "native-country    0\n",
       "income            0\n",
       "Income_Over50     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove leading and trailing spaces from column names, like \" income\"\n",
    "income.columns = income.columns.str.strip()\n",
    "\n",
    "# Dummify our target variable to be 1 or 0, 1 for people who make over 50k in income\n",
    "income['Income_Over50'] = (income['income'] == ' >50K' ).astype(int)\n",
    "\n",
    "# Ensure that the dummification worked to keep same distribution of >50k income's\n",
    "income['Income_Over50'].value_counts()\n",
    "\n",
    "# Both value count functions display the same 24,720 people making below 50k compared to 7,841 making above\n",
    "income['income'].value_counts()\n",
    "\n",
    "# Check to ensure that all the variable datatypes are as expected (numerical columns are Int, categorical are Object)\n",
    "income.dtypes\n",
    "\n",
    "# Check for missing values before modeling, displaying no issues \n",
    "income.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4fe30e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>Income_Over50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  Income_Over50  \n",
       "0          2174             0              40   United-States              0  \n",
       "1             0             0              13   United-States              0  \n",
       "2             0             0              40   United-States              0  \n",
       "3             0             0              40   United-States              0  \n",
       "4             0             0              40            Cuba              0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicate target column\n",
    "income_final = income.drop(['income'], axis = 1)\n",
    "\n",
    "# Replace '?' with 'Unknown' in the all column's\n",
    "income_final['workclass'] = income_final['workclass'].replace(' ?', 'Unknown')\n",
    "income_final['occupation'] = income_final['occupation'].replace(' ?', 'Unknown')\n",
    "income_final['native-country'] = income_final['native-country'].replace(' ?', 'Unknown')\n",
    "\n",
    "# Make the ?'s into Unknown for workclass just in case the model cannot handle it\n",
    "income_final['workclass'] = income_final['workclass'].replace('?', 'Unknown')\n",
    "\n",
    "# View final dataset before modeling to ensure data is prepared\n",
    "income_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a31626",
   "metadata": {},
   "source": [
    "#### Before modeling, we must ask ourselves a few questions:\n",
    "1. Do we need all predictors? For example, both education and education-num, or does one provide enough predictive power without the other?\n",
    "2. Is each fnlwgt's a unique value that doesn't provide predictability, or are these values simply a continous spectrum?\n",
    "3. What is the best options of modeling methods in this case, besides Naive Bayes and Neural Networks.\n",
    "4. What scoring metrics are appropriate for model validation?\n",
    "\n",
    "To test the predictability of the education feature's we will have to run identical model types including each variable without the other. And since education and education-num are perfectly collinear (meaning they convey the same information), we cannot include both in our models.  Also, after looking to the data dictionary, `fnlwgt` is labeled as a continuous variable numerically representing an individual's education which provided no predictive power to our model.  As for the last point, we know that **XGBoost can be used to deal with imbalanced target variables**, as we have here with only 25% of the people making over 50k, indicating it may be a good use case.  Also, **creating a stacked model that combines the predictability of several models could prove successful**.  Lastly, it our options for scoring metrics include the classification associated scores like accuracy, precision, recall and f1-score.  Like mentioned earlier, it is typically advised to avoid accuracy when faced with unbalanced distribution of our target class, while precision and recall typically are used in specific scenarios in which we need to more accuratley predict the target, for example when working with fraud or cancer testing data.  In this case, **f1 is a more hollistic option, which will display a tradeoff between recall and precision, so what % we are able to accurately predict of people making over 50k, and at what success rate.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe5ebb",
   "metadata": {},
   "source": [
    "### Feature Selection & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "078bd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and Y sets, dropping relationship results in better scoring\n",
    "X = income_final.drop([\"Income_Over50\",'relationship','sex','education','fnlwgt'], axis = 1)\n",
    "y = income_final[\"Income_Over50\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "# Define column transformer for preprocessing models like NeuralNetworks, XgBoost\n",
    "ct_all = ColumnTransformer(\n",
    "  [\n",
    "    (\"dummify\", OneHotEncoder(sparse_output = False, handle_unknown='ignore'),\n",
    "    make_column_selector(dtype_include=object)),\n",
    "    (\"standardize\", StandardScaler(), make_column_selector(dtype_include=np.int64))\n",
    "  ],\n",
    "  remainder = \"passthrough\" \n",
    ").set_output(transform = \"pandas\")\n",
    "\n",
    "# Define a column transformer to ensure all X's are encoded to be between 0 and 1, to remove -1 input error for NB\n",
    "ct_nb = ColumnTransformer(\n",
    "    [\n",
    "        (\"dummify\", OneHotEncoder(sparse_output=False, handle_unknown='ignore'),\n",
    "        make_column_selector(dtype_include=object)),\n",
    "        (\"scale_to_non_negative\", MinMaxScaler(), make_column_selector(dtype_include=np.number))\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb70f6",
   "metadata": {},
   "source": [
    "## Model 1: Naive Bayes (Categorical)\n",
    "\n",
    "Naive Bayes is a simple yet effective classification algorithm, meaning that it is only applied to categorical data. It operates under the 'naive' statistical assumption that features are independent of one another, which simplifies calculations and results in fast predictions. Despite its simplicity, Naive Bayes often performs well on real-world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1e31b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'naivebayesclassifier__alpha': 0.01, 'naivebayesclassifier__fit_prior': True, 'naivebayesclassifier__min_categories': 1}\n",
      "\n",
      " Naive Bayes - F1 Score:  0.726688402425609\n"
     ]
    }
   ],
   "source": [
    "# NB Pipeline\n",
    "naive_bayes_pipeline = Pipeline([\n",
    "    (\"preprocessor\", ct_nb),\n",
    "    (\"naivebayesclassifier\", CategoricalNB())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for CategoricalNB\n",
    "param_grid = {\n",
    "    'naivebayesclassifier__alpha': [1.0, 0.5, 0.1, 0.01],  \n",
    "    'naivebayesclassifier__min_categories': [1, 2, 4, None],\n",
    "    'naivebayesclassifier__fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Tuning with GridSearchCV, using F1 score as the scoring metric for classification\n",
    "grid_search = GridSearchCV(naive_bayes_pipeline, param_grid, cv=5, scoring='f1_macro')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = grid_search.predict(X_test)\n",
    "test_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "\n",
    "print(\"\\n Naive Bayes - F1 Score: \", test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad492eb",
   "metadata": {},
   "source": [
    "Our Naive Bayes Classifier, was able to predict the dataset moderately well with an F1-score of 0.7266.  This means **that our model can accurately predict around 73% of the people who make over 50k in Income**. As a general guideline, an F1 score of 0.7 or higher is often considered good, so we are right on this threshold.  Using the categorical NB classifier was the proper choice here, as our target variable and all variables can be thought of as categories, besides maybe final_weights or `ages`. I ran a few NB model's, removing `education` and `education-num`, and discovered a clear decline in predictability of our model, suggesting that both are needed in our feature set, and that education has a positive impact on Income. Based on intuition I also iterated through a few NB model's with different feature sets, finding that removing `relationship` and `sex` from our model provides more predictability from 0.7 to 0.7266.  This makes conceptual sense, as once's relationship status could contribute to their Income, however it shouldn't really as your income is an indivdually-earned and associated value. And of course, sexual discrimination is not allowed in the workplace so someone's sex should not impact the amount of money they earn, confirmed by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda9410",
   "metadata": {},
   "source": [
    "## Model 2: Neural Networks MLPClassifier\n",
    "\n",
    "In our second attempt, we leveraged a Neural Network model, which are powerful models capable of capturing complex relationships in the data by mimicking the way our brain detects patterns. They consist of layers of interconnected nodes, or 'neurons,' that work together to learn patterns. While they require more computational resources than simpler models like Naive Bayes, they often achieve higher accuracy, particularly when working with large datasets with constant information to teach our model what traits attribute to a person making over 50k a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9376134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'mlpclassifier__activation': 'relu', 'mlpclassifier__hidden_layer_sizes': (100,), 'mlpclassifier__learning_rate': 'adaptive', 'mlpclassifier__solver': 'adam'}\n",
      "\n",
      " Neural Networks - F1 Score:  0.7996550233033553\n"
     ]
    }
   ],
   "source": [
    "# MLP Pipeline - 1\n",
    "neural_network = Pipeline([\n",
    "    (\"preprocessor\", ct_all),\n",
    "    (\"mlpclassifier\", MLPClassifier(max_iter = 1000, early_stopping = True))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for tuning, including solver, 1 hidden layer with 100 units, learning rate and activation functions\n",
    "param_grid = {\n",
    "    'mlpclassifier__hidden_layer_sizes': [(100,)],  \n",
    "    'mlpclassifier__activation': ['relu', 'tanh', 'logistic','identity'],\n",
    "    'mlpclassifier__solver':['adam','sgd'],                               # lbfgs resulted in failure to converge\n",
    "    'mlpclassifier__learning_rate': ['constant','invscaling','adaptive']\n",
    "}\n",
    "\n",
    "# Tuning with GridSearchCV, using r2 as the scoring metric for classification\n",
    "grid_search = GridSearchCV(neural_network, param_grid, cv=5, scoring='f1_macro')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions_nn = grid_search.predict(X_test)\n",
    "test_f1_nn = f1_score(y_test, test_predictions_nn, average='macro')\n",
    "\n",
    "print(\"\\n Neural Networks - F1 Score: \", test_f1_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc64061",
   "metadata": {},
   "source": [
    "Our Neural Network Classifier, showed significant improvement in its ability to predict those making over 50 thousand dollars, with a new F1-score of nearly 0.80. When referring to our general guideline, **an F1 score of 0.8 is classified as great, meaning that we are predicting a large % of the target class correctly, and of those predicted to make over 50k, we are not receiving many false positives or untrue predictions.**  In terms of the tuned parameters, the 'adam' solver is used as the default within MLP as it typically results in the best predictions, which makes sense why it was our best solver.  The rectified linear unit was our optimal activation function,  suggesting that the data may follow a pattern in which a specific age or combination of predictors, results in an increase or decrease in Income or those making 50k.  From the previous NN model's I have ran, the 'constant' learning rate is best paired with the adam solver, with some options like adaptive, only being applicable for specific situations.  The best model capable of running quickly include one hidden layer, with 100 hidden units, that was able to adjust our weights and accurately predict those making over 50k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28d6f3",
   "metadata": {},
   "source": [
    "## Model 3: XGBoosting Tree\n",
    "\n",
    "In our final model, we applied XGBoosting, which stands for Extreme Gradient Boosting. XGBoost is a highly efficient and powerful implementation of the gradient boosting algorithm, known for its ability to handle large datasets and complex data patterns. It works by creating an ensemble of decision trees, where each subsequent tree aims to correct the errors of the previous ones, enhancing the model's predictive accuracy. XGBoost is often the go-to choice for machine learning tasks due to its robustness and flexibility, making it a fitting choice for our imbalanced classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "89bc8f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'classifier__learning_rate': 0.1, 'classifier__max_depth': 4, 'classifier__n_estimators': 200, 'classifier__scale_pos_weight': 1.305}\n",
      "\n",
      " XGBoost - F1 Score:  0.7280779450841453\n"
     ]
    }
   ],
   "source": [
    "# Define a grid of possible weight values that are close to distribution of imbalance target 24,720 / 7,841 = 3.15\n",
    "pos_weight_values = [1.30, 1.305]\n",
    "    \n",
    "# Setup pipeline #1 for XGBoost\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"preprocessor\", ct_all),\n",
    "    (\"classifier\", XGBClassifier(use_label_encoder=False, eval_metric='logloss'))  # Parameters needed to suppress warnings\n",
    "])\n",
    "\n",
    "# Parameter grid for XGBoost with different parameters being tuned\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [100,200,300],             \n",
    "    'classifier__learning_rate': [0.1, 0.3, 0.5],    \n",
    "    'classifier__max_depth': [3,4],                  \n",
    "    'classifier__scale_pos_weight': pos_weight_values\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV for XGBoost                               # n_jobs -1 tells Jupyter to user all CPU's for gridsearch\n",
    "grid_search_xgb = GridSearchCV(xgb_pipeline, param_grid_xgb, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "# Fitting our model on training data\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters: \", grid_search_xgb.best_params_)\n",
    "\n",
    "# Predict on the test set\n",
    "test_xgb_predictions = grid_search_xgb.predict(X_test)\n",
    "\n",
    "# Use f1_score function\n",
    "test_f1_xgb = f1_score(y_test, test_xgb_predictions)\n",
    "\n",
    "print(\"\\n XGBoost - F1 Score: \", test_f1_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff860419",
   "metadata": {},
   "source": [
    "The Extreme Gradient Boosting Model (XGBoost) surprisingly did not outperform the other models in this analysis, achieving an F1 score of 0.728. While the inclusion of the `scale_pos_weight parameter`, which adjusts for class imbalance, was intended to maximize the trade-off between precision and recall, it did not result in the highest performance. In this case, the parameter was set to 1.305, reflecting the ratio of individuals earning less than 50k to those earning more. This ratio, being above 1, indicates an imbalanced class problem, which is a challenge that XGBoost typically handles well.\n",
    "The other best parameters identified for the XGBoost model were a `learning_rate` of 0.1,`max_depth` of 4, and `n_estimators` of 300. However, despite these settings, the XGBoost model's performance was on par with the Naive Bayes model and did not exceed the performance of the Neural Network, which achieved a higher F1 score. This outcome suggests that **while XGBoost is a powerful and flexible model, in this particular income classification task, it did not outperform the simpler models or the more complex Neural Network**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b343b5d",
   "metadata": {},
   "source": [
    "## Model 4: Stacked Classifier\n",
    "Neural Network, XgBoost, Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2b8db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models within the preprocessing pipeline, and tuning with optimal values from Models 1-3\n",
    "\n",
    "base_models = [\n",
    "    ('gradient_boosting', Pipeline([\n",
    "        ('preprocessor', ct_all),\n",
    "        ('classifier', GradientBoostingClassifier(learning_rate=0.1, max_depth=4, n_estimators=300))\n",
    "    ])),\n",
    "    ('xgbboost', Pipeline([\n",
    "        ('preprocessor', ct_all),\n",
    "        ('classifier', XGBClassifier(learning_rate=0.1, max_depth=4, n_estimators=300,\n",
    "                                                  scale_pos_weight=1.305))\n",
    "    ])),\n",
    "    ('mlp_classifier', Pipeline([\n",
    "        ('preprocessor', ct_all),\n",
    "        ('classifier', MLPClassifier(activation='relu', hidden_layer_sizes=(100,), \n",
    "                                     learning_rate='constant', solver='adam', max_iter=1000))\n",
    "    ])),\n",
    "    ('categorical_nb', Pipeline([\n",
    "        ('preprocessor', ct_nb),\n",
    "        ('classifier', CategoricalNB(alpha=0.5, fit_prior=True, min_categories=1))\n",
    "    ]))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74704986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model - F1 Score:  0.8199020389171616\n"
     ]
    }
   ],
   "source": [
    "# Define the final regressor method, typically Linear or Logistic, in this case for classification use Log\n",
    "final_regressor = LogisticRegression()\n",
    "\n",
    "# Create the stacking ensemble\n",
    "stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=final_regressor, cv=5)\n",
    "\n",
    "# Fit the stacking model on train data\n",
    "stacking_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict/evaluate the model on Test data\n",
    "stacking_predictions = stacking_classifier.predict(X_test)\n",
    "stacking_f1 = f1_score(y_test, stacking_predictions, average = 'macro')\n",
    "\n",
    "# Stacking model performed best \n",
    "print(\"Stacking Model - F1 Score: \", stacking_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0753ecf",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "Finally by utilizing our new method of a stacked model, which includes iterative learning from multiple selected models, we were able to **increase our F1 to be 0.822**, to be above the threshold of 0.8 and **indicating that our model both accurately and precisely predicts those making over 50k a year**.  To achieve the best results, we combined the predictive power of our XGBoosting Tree, Categorical Naive Bayes, Neural Network, and Gradient Boosting model to maximize our performance metric, F1.  We settled upon a simple yet powerful final feature set, consisting of the following variables used to predict if Income is greater than 50 thousand or not:\n",
    "\n",
    "1. age\t\n",
    "2. workclass\t\n",
    "3. fnlwgt\t\n",
    "4. education\t\n",
    "5. education-num\t\n",
    "6. marital-status\n",
    "7. occupation\t\n",
    "8. race\t            \n",
    "9. capital-gain\t\n",
    "10. capital-loss\t\n",
    "11. hours-per-week\t\n",
    "12. native-country\n",
    "\n",
    "In terms of some specific effects of these variables on income, it can be intuitively confirmed that variables like working hours-per-week, education, and capital gain, have a strongly positive relationship with income.  While on the other hand, capital loss likely has a strong negative association with income.  However, the remaining predictors specifically like Age and Education, differ based on each person and their situations, resulting in unclear and more complex effects on income.  For example, some millionaires may not have gone to college and reach this wealth at a young age, which makes predicting income a bit more complex than one may assume before diving into some data.  \n",
    "\n",
    "As for a next step, it would be useful to find a few additional predictors to include in this dataset, potentially related to their workplace attitude or personality.  It would also be beneficial to dive further into the effects of each complex feature to derive richer insights, such as why race has any effect on income, as this is discriminatory and should not be seen in data.  The same would be interesting to know about other variables such as, marital-status.  For instance, does being married contribute to more wealth if spouse's share revenue, or does it make for additional costs associated with the relationship?  However, for now we are able to accurately predict Income (>50k) with a final F1 score of 87., and use this model in segmentation, targeted marketing, and other financial use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd052cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
