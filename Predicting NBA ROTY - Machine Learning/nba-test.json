{
	"jobConfig": {
		"name": "nba-test",
		"description": "",
		"role": "arn:aws:iam::943686807189:role/service-role/AWSGlueServiceRole",
		"command": "pythonshell",
		"version": "3.0",
		"runtime": null,
		"workerType": null,
		"numberOfWorkers": null,
		"maxCapacity": 1,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "nba-test.py",
		"scriptLocation": "s3://aws-glue-assets-943686807189-us-west-1/scripts/",
		"language": "python-3.9",
		"spark": false,
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-03-06T21:04:42.799Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-943686807189-us-west-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"pythonShellPrebuiltLibraryOption": "analytics",
		"flexExecution": false,
		"minFlexWorkers": null
	},
	"hasBeenSaved": false,
	"script": "import requests\nimport pandas as pd\nfrom datetime import date, timedelta\nimport boto3\nfrom io import StringIO\n\ndef fetch_data_for_date(url, headers, target_date):\n    all_data = []\n    next_cursor = None\n\n    while True:\n        paginated_url = url + target_date\n        if next_cursor:\n            paginated_url += f\"&cursor={next_cursor}\"\n\n        response = requests.get(paginated_url, headers=headers)\n        if response.status_code == 200:\n            data = response.json()\n            if not data['data']:\n                break\n            all_data.extend(data['data'])\n            if 'meta' in data and 'next_cursor' in data['meta']:\n                next_cursor = data['meta']['next_cursor']\n            else:\n                break\n        else:\n            print(\"Failed to fetch data:\", response.status_code)\n            break\n\n    return all_data\n\ndef save_to_s3(data, bucket_name, file_name):\n    s3 = boto3.client('s3')\n    csv_buffer = StringIO()\n    pd.DataFrame(data).to_csv(csv_buffer, index=False)\n    csv_buffer.seek(0)\n    s3.put_object(Bucket=bucket_name, Key=file_name, Body=csv_buffer.getvalue())\n\ndef read_from_s3(bucket_name, file_name):\n    s3 = boto3.client('s3')\n    obj = s3.get_object(Bucket=bucket_name, Key=file_name)\n    df = pd.read_csv(obj['Body'])\n    return df\n\nbase_url = \"https://api.balldontlie.io/v1/stats?dates[]=\"\nheaders = {\n    \"Authorization\": \"b550c0f1-670c-48dd-aba9-81e75cdd43b7\"\n}\n\nyesterday = date.today() - timedelta(days=1)\nyesterday_date = yesterday.strftime(\"%Y-%m-%d\")\n\ndata = fetch_data_for_date(base_url, headers, yesterday_date)\n\nif data:\n    df_new = pd.json_normalize(data)\n    bucket_name = \"rmb-glue\"\n    current_year_file_name = \"Current_Year.csv\"\n    \n    try:\n        # Read existing data from S3\n        existing_df = read_from_s3(bucket_name, current_year_file_name)\n        # Concatenate new data with existing data\n        updated_df = pd.concat([existing_df, df_new], ignore_index=True)\n        # Save updated data back to S3\n        save_to_s3(updated_df.to_dict(orient='records'), bucket_name, current_year_file_name)\n        print(f\"Fetched data for yesterday: {yesterday_date}. Data appended to 'Current_Year.csv' in S3.\")\n    except Exception as e:\n        print(\"Error:\", e)\nelse:\n    print(\"No data available for yesterday:\", yesterday_date)\n"
}